;*************************  memcpy32.asm  ************************************
; Author:           Agner Fog
; Date created:     2008-07-18
; Last modified:    2008-07-18
; Description:
; Faster version of the standard memcpy function:
; void * A_memcpy(void *dest, const void *src, size_t count);
; Copies 'count' bytes from 'src' to 'dest'
;
; Overriding standard function memcpy:
; The alias ?OVR_memcpy is changed to _memcpy in the object file if
; it is desired to override the standard library function memcpy.
;
; Position-independent code is generated if POSITIONINDEPENDENT is defined.
;
; Optimization:
; Uses XMM registers to copy 16 bytes at a time, aligned.
; If source and destination are misaligned relative to each other
; then the code will combine parts of every two consecutive 16-bytes 
; blocks from the source into one 16-bytes register which is written 
; to the destination, aligned.
; This method is 2 - 6 times faster than the implementations in the
; standard C libraries (MS, Gnu) when src or dest are misaligned.
; When src and dest are aligned by 16 (relative to each other) then this
; function is only slightly faster than the best standard libraries.
; CPU dispatching included for 386, SSE2 and Suppl-SSE3 instruction sets.
;
; Future extensions:
; Implementations for the AMD SSE5 and Intel AVX instruction sets are 
; illustrated in www.agner.org/optimize/asmexamples.zip
;
; Copyright (c) 2008 GNU General Public License www.gnu.org/licenses/gpl.html
;******************************************************************************
.686
.xmm
.model flat

public _A_memcpy                       ; Function A_memcpy
public ?OVR_memcpy                     ; ?OVR removed if standard function memcpy overridden
public $memcpyEntry2                   ; Entry from memmove
public _CacheBypassLimit               ; Bypass cache if count > _CacheBypassLimit


; Imported from instrset32.asm
extern _InstructionSet: near           ; Instruction set for CPU dispatcher

; Define return from this function
RETURNM MACRO
IFDEF   POSITIONINDEPENDENT
        pop     ebx
ENDIF
        pop     edi
        pop     esi
        mov     eax, [esp+4]           ; Return value = dest
        ret
ENDM

; Macro for arbitrary instruction with position-independent reference
PICREFERENCE MACRO TARGET, REFPOINT, BYTE1, BYTE2, BYTE3
; Make position-independent instrution of the form
; add eax, [ebx+TARGET-REFPOINT]
; where ebx contains the address of REFPOINT
; BYTE1, BYTE2, BYTE3 are the first 2 or 3 bytes of the instruction
; add eax, [ebx+????]
; (as obtained from an assembly listing) including opcode byte, 
; mod/reg/rm byte and possibly sib byte, but not including the 4-bytes 
; offset which is generated by this macro.
; Any instruction and any registers can be coded into BYTE1, BYTE2, BYTE3.

local p0, p1, p2
; Insert byte codes except last one
p0:
   db  BYTE1
IFNB  <BYTE3>      ; if BYTE3 not blank
   db  BYTE2
ENDIF   
p1:   
; Make bogus CALL instruction for making self-relative reference.
; This is the only way the assembler can make a self-relative reference
   call near ptr TARGET + (p2-REFPOINT) 
p2:
; back-patch CALL instruction to change it to the desired instruction
; by replacing CALL opcode by the last byte of desired instruction
   org p1
IFNB  <BYTE3>      ; if BYTE3 not blank
   db  BYTE3
ELSE
   db  BYTE2
ENDIF
   org p2          ; Back to end of instruction
ENDM 


.code

; extern "C" void * A_memcpy(void * dest, const void * src, size_t count);
; Function entry:
_A_memcpy PROC    NEAR
?OVR_memcpy LABEL NEAR
        push    esi
        push    edi
        mov     edi, [esp+12]          ; dest
        mov     esi, [esp+16]          ; src
        mov     ecx, [esp+20]          ; count
$memcpyEntry2 label near               ; Entry from memmove

IFNDEF  POSITIONINDEPENDENT
        jmp     [memcpyDispatch]       ; Go to appropriate version, depending on instruction set
RP      equ     0                      ; RP = 0 if not position-independent

ELSE    ; Position-independent code

        push    ebx
        call    get_thunk_ebx          ; get reference point for position-independent code
RP:                                    ; reference point ebx = offset RP
A020:                                  ; Go here after CPU dispatching

; Make the following instruction with address relative to RP:
; cmp [ebx-RP+memcpyCPUVersion], 1
PICREFERENCE memcpyCPUVersion, RP, 83H, 0BBH
        db      1                      ; Last byte of cmp instruction
        
        jb      memcpyCPUDispatch      ; First time: memcpyCPUVersion = 0, go to dispatcher
        je      memcpy386              ; memcpyCPUVersion = 1, go to 80386 version

ENDIF
        
        
memcpySSE2: ; SSE2 and later versions begin here        
        cmp     ecx, 40H
        jae     B100                   ; Use simpler code if count < 64
        
        ; count < 64. Move 32-16-8-4-2-1 bytes
        add     esi, ecx               ; end of src
        add     edi, ecx               ; end of dest
        neg     ecx                    ; negative index from the end
        cmp     ecx, -20H
        jg      A100        
        ; move 32 bytes
        ; movq is faster than movdqu on current processors (2008),
        ; movdqu may be faster on future processors
        movq    xmm0, qword ptr [esi+ecx]
        movq    xmm1, qword ptr [esi+ecx+8]
        movq    xmm2, qword ptr [esi+ecx+10H]
        movq    xmm3, qword ptr [esi+ecx+18H]
        movq    qword ptr [edi+ecx], xmm0
        movq    qword ptr [edi+ecx+8], xmm1
        movq    qword ptr [edi+ecx+10H], xmm2
        movq    qword ptr [edi+ecx+18H], xmm3
        add     ecx, 20H
A100:   cmp     ecx, -10H        
        jg      A200
        ; move 16 bytes
        movq    xmm0, qword ptr [esi+ecx]
        movq    xmm1, qword ptr [esi+ecx+8]
        movq    qword ptr [edi+ecx], xmm0
        movq    qword ptr [edi+ecx+8], xmm1
        add     ecx, 10H
A200:   cmp     ecx, -8        
        jg      A300
        ; move 8 bytes
        movq    xmm0, qword ptr [esi+ecx]
        movq    qword ptr [edi+ecx], xmm0
        add     ecx, 8
A300:   cmp     ecx, -4        
        jg      A400
        ; move 4 bytes
        mov     eax, [esi+ecx]
        mov     [edi+ecx], eax
        add     ecx, 4
        jz      A900                     ; early out if count divisible by 4
A400:   cmp     ecx, -2        
        jg      A500
        ; move 2 bytes
        movzx   eax, word ptr [esi+ecx]
        mov     [edi+ecx], ax
        add     ecx, 2
A500:   cmp     ecx, -1
        jg      A900        
        ; move 1 byte
        movzx   eax, byte ptr [esi+ecx]
        mov     [edi+ecx], al
A900:   ; finished
        RETURNM        
        
B100:   ; count >= 64
        ; Note: this part will not always work if count < 64
        ; Calculate size of first block up to first regular boundary of dest
        mov     edx, edi
        neg     edx
        and     edx, 0FH
        jz      B300                    ; Skip if dest aligned by 16
        
        ; edx = size of first partial block, 1 - 15 bytes
        add     esi, edx
        add     edi, edx
        sub     ecx, edx
        neg     edx
        cmp     edx, -8
        jg      B200
        ; move 8 bytes
        movq    xmm0, qword ptr [esi+edx]
        movq    qword ptr [edi+edx], xmm0
        add     edx, 8
B200:   cmp     edx, -4        
        jg      B210
        ; move 4 bytes
        mov     eax, [esi+edx]
        mov     [edi+edx], eax
        add     edx, 4
B210:   cmp     edx, -2        
        jg      B220
        ; move 2 bytes
        movzx   eax, word ptr [esi+edx]
        mov     [edi+edx], ax
        add     edx, 2
B220:   cmp     edx, -1
        jg      B300
        ; move 1 byte
        movzx   eax, byte ptr [esi+edx]
        mov     [edi+edx], al
        
B300:   ; Now dest is aligned by 16. Any partial block has been moved        
        ; Find alignment of src modulo 16 at this point:
        mov     eax, esi
        and     eax, 0FH
        
        ; Set up for loop moving 32 bytes per iteration:
        mov     edx, ecx               ; Save count
        and     ecx, -20H              ; Round down to nearest multiple of 32
        add     esi, ecx               ; Point to the end
        add     edi, ecx               ; Point to the end
        sub     edx, ecx               ; Remaining data after loop
        sub     esi, eax               ; Nearest preceding aligned block of src

IFNDEF  POSITIONINDEPENDENT
        ; Check if count very big
        cmp     ecx, [_CacheBypassLimit]
        ja      B400                   ; Use non-temporal store if count > _CacheBypassLimit
        neg     ecx                    ; Negative index from the end
        
        ; Dispatch to different codes depending on src alignment
        jmp     AlignmentDispatch[eax*4]

B400:   neg     ecx
        ; Dispatch to different codes depending on src alignment
        jmp     AlignmentDispatchNT[eax*4]

ELSE    ; Position-independent code

        ; Check if count very big
        ; Make the following instruction with address relative to RP:
        ; cmp     ecx, [ebx-RP+_CacheBypassLimit]
        PICREFERENCE _CacheBypassLimit, RP, 3BH, 8BH
        ja      B400                   ; Use non-temporal store if count > _CacheBypassLimit
        neg     ecx                    ; Negative index from the end
        
        ; Dispatch to different codes depending on src alignment
        

        ; AlignmentDispatch table contains addresses relative to RP
        ; Add table entry to ebx=RP to get jump address.

        ; Make the following instruction with address relative to RP:
        ; add ebx,[ebx-RP+AlignmentDispatch+eax*4]
        PICREFERENCE AlignmentDispatch, RP, 03H, 9CH, 83H
        jmp     ebx
        
B400:   neg     ecx

        ; Same with AlignmentDispatchNT:        
        PICREFERENCE AlignmentDispatchNT, RP, 03H, 9CH, 83H
        jmp     ebx        
ENDIF

align   16
C100:   ; Code for aligned src. SSE2 or later instruction set
        ; The nice case, src and dest have same alignment.

        ; Loop. ecx has negative index from the end, counting up to zero
        movaps  xmm0, [esi+ecx]
        movaps  xmm1, [esi+ecx+10H]
        movaps  [edi+ecx], xmm0
        movaps  [edi+ecx+10H], xmm1
        add     ecx, 20H
        jnz     C100
        
        ; Move the remaining edx bytes (0 - 31):
        add     esi, edx
        add     edi, edx
        neg     edx
        jz      C500                   ; Skip if no more data
        ; move 16-8-4-2-1 bytes, aligned
        cmp     edx, -10H
        jg      C200
        ; move 16 bytes
        movaps  xmm0, [esi+edx]
        movaps  [edi+edx], xmm0
        add     edx, 10H
C200:   cmp     edx, -8
        jg      C210        
        ; move 8 bytes
        movq    xmm0, qword ptr [esi+edx]
        movq    qword ptr [edi+edx], xmm0
        add     edx, 8 
        jz      C500                   ; Early skip if count divisible by 8       
C210:   cmp     edx, -4
        jg      C220        
        ; move 4 bytes
        mov     eax, [esi+edx]
        mov     [edi+edx], eax
        add     edx, 4        
C220:   cmp     edx, -2
        jg      C230        
        ; move 2 bytes
        movzx   eax, word ptr [esi+edx]
        mov     [edi+edx], ax
        add     edx, 2
C230:   cmp     edx, -1
        jg      C500        
        ; move 1 byte
        movzx   eax, byte ptr [esi+edx]
        mov     [edi+edx], al
C500:   ; finished     
        RETURNM
        
       
; Code for each src alignment, SSE2 instruction set:
; Make separate code for each alignment u because the shift instructions
; have the shift count as a constant:

MOVE_UNALIGNED_SSE2 MACRO u, nt
; Move ecx + edx bytes of data
; Source is misaligned. (src-dest) modulo 16 = u
; nt = 1 if non-temporal store desired
; eax = u
; esi = src - u = nearest preceding 16-bytes boundary
; edi = dest (aligned)
; ecx = - (count rounded down to nearest divisible by 32)
; edx = remaining bytes to move after loop
LOCAL L1, L2
        movdqa  xmm0, [esi+ecx]        ; Read from nearest preceding 16B boundary
L1:    ; Loop. ecx has negative index from the end, counting up to zero
        	movdqa  xmm1, [esi+ecx+10H]    ; Read next two blocks aligned
        movdqa  xmm2, [esi+ecx+20H]
        movdqa  xmm3, xmm1             ; Copy because used twice
        psrldq  xmm0, u                ; shift right
        pslldq  xmm1, 16-u             ; shift left
        por     xmm0, xmm1             ; combine blocks
        IF nt eq 0
        movdqa  [edi+ecx], xmm0        ; Save aligned
        ELSE
        movntdq [edi+ecx], xmm0        ; non-temporal save
        ENDIF
        movdqa  xmm0, xmm2             ; Save for next iteration
        psrldq  xmm3, u                ; shift right
        pslldq  xmm2, 16-u             ; shift left
        por     xmm3, xmm2             ; combine blocks
        IF nt eq 0
        movdqa  [edi+ecx+10H], xmm3    ; Save aligned
        ELSE
        movntdq [edi+ecx+10H], xmm3    ; non-temporal save
        ENDIF
        add     ecx, 20H               ; Loop through negative values up to zero
        jnz     L1
        
        ; Set up for edx remaining bytes
        add     esi, edx
        add     edi, edx
        neg     edx
        cmp     edx, -10H
        jg      L2
        ; One more 16-bytes block to move
        movdqa  xmm1, [esi+edx+10H]
        psrldq  xmm0, u                ; shift right
        pslldq  xmm1, 16-u             ; shift left
        por     xmm0, xmm1             ; combine blocks
        IF nt eq 0
        movdqa  [edi+edx], xmm0        ; Save aligned
        ELSE
        movntdq [edi+edx], xmm0        ; non-temporal save
        ENDIF        
        add     edx, 10H        
L2:     ; Get src pointer back to misaligned state
        add     esi, eax
        ; Move remaining 0 - 15 bytes, unaligned
        jmp     C200
ENDM

MOVE_UNALIGNED_SSE2_4 MACRO nt
; Special case for u = 4
LOCAL L1, L2
        movaps  xmm0, [esi+ecx]        ; Read from nearest preceding 16B boundary
L1:     ; Loop. ecx has negative index from the end, counting up to zero
        movaps  xmm1, [esi+ecx+10H]    ; Read next two blocks aligned
        movss   xmm0, xmm1             ; Moves 4 bytes, leaves remaining bytes unchanged
        pshufd  xmm0, xmm0, 00111001B
        IF nt eq 0
        movdqa  [edi+ecx], xmm0        ; Save aligned
        ELSE
        movntdq [edi+ecx], xmm0        ; Non-temporal save
        ENDIF
        movaps  xmm0, [esi+ecx+20H]
        movss   xmm1, xmm0
        pshufd  xmm1, xmm1, 00111001B
        IF nt eq 0
        movdqa  [edi+ecx+10H], xmm1    ; Save aligned
        ELSE
        movntdq [edi+ecx+10H], xmm1    ; Non-temporal save
        ENDIF
        add     ecx, 20H               ; Loop through negative values up to zero
        jnz     L1        
        ; Set up for edx remaining bytes
        add     esi, edx
        add     edi, edx
        neg     edx
        cmp     edx, -10H
        jg      L2
        ; One more 16-bytes block to move
        movaps  xmm1, [esi+edx+10H]    ; Read next two blocks aligned
        movss   xmm0, xmm1
        pshufd  xmm0, xmm0, 00111001B
        IF nt eq 0
        movdqa  [edi+edx], xmm0        ; Save aligned
        ELSE
        movntdq [edi+edx], xmm0        ; Non-temporal save
        ENDIF
        add     edx, 10H        
L2:     ; Get src pointer back to misaligned state
        add     esi, eax
        ; Move remaining 0 - 15 bytes, unaligned
        jmp     C200
ENDM 

MOVE_UNALIGNED_SSE2_8 MACRO nt
; Special case for u = 8
LOCAL L1, L2
        movaps  xmm0, [esi+ecx]        ; Read from nearest preceding 16B boundary
L1:     ; Loop. ecx has negative index from the end, counting up to zero
        movaps  xmm1, [esi+ecx+10H]    ; Read next two blocks aligned
        movsd   xmm0, xmm1             ; Moves 8 bytes, leaves remaining bytes unchanged
        shufps  xmm0, xmm0, 01001110B  ; Rotate
        IF nt eq 0
        movdqa  [edi+ecx], xmm0        ; Save aligned
        ELSE
        movntdq [edi+ecx], xmm0        ; Non-temporal save
        ENDIF
        movaps  xmm0, [esi+ecx+20H]
        movsd   xmm1, xmm0
        shufps  xmm1, xmm1, 01001110B
        IF nt eq 0
        movdqa  [edi+ecx+10H], xmm1    ; Save aligned
        ELSE
        movntdq [edi+ecx+10H], xmm1    ; Non-temporal save
        ENDIF
        add     ecx, 20H               ; Loop through negative values up to zero
        jnz     L1        
        ; Set up for edx remaining bytes
        add     esi, edx
        add     edi, edx
        neg     edx
        cmp     edx, -10H
        jg      L2
        ; One more 16-bytes block to move
        movaps  xmm1, [esi+edx+10H]    ; Read next two blocks aligned
        movsd   xmm0, xmm1
        shufps  xmm0, xmm0, 01001110B
        IF nt eq 0
        movdqa  [edi+edx], xmm0        ; Save aligned
        ELSE
        movntdq [edi+edx], xmm0        ; Non-temporal save
        ENDIF
        add     edx, 10H        
L2:     ; Get src pointer back to misaligned state
        add     esi, eax
        ; Move remaining 0 - 15 bytes, unaligned
        jmp     C200
ENDM 

MOVE_UNALIGNED_SSE2_12 MACRO nt
; Special case for u = 12
LOCAL L1, L2
        movaps  xmm0, [esi+ecx]        ; Read from nearest preceding 16B boundary
        pshufd  xmm0, xmm0, 10010011B
L1:     ; Loop. ecx has negative index from the end, counting up to zero
        movaps  xmm1, [esi+ecx+10H]    ; Read next two blocks aligned
        movaps  xmm2, [esi+ecx+20H]
        pshufd  xmm1, xmm1, 10010011B
        pshufd  xmm2, xmm2, 10010011B
        movaps  xmm3, xmm2
        movss   xmm2, xmm1             ; Moves 4 bytes, leaves remaining bytes unchanged
        movss   xmm1, xmm0             ; Moves 4 bytes, leaves remaining bytes unchanged       
        IF nt eq 0
        movdqa  [edi+ecx], xmm1        ; Save aligned
        movdqa  [edi+ecx+10H], xmm2    ; Save aligned
        ELSE
        movntdq [edi+ecx], xmm1        ; Non-temporal save
        movntdq [edi+ecx+10H], xmm2    ; Non-temporal save
        ENDIF
        movaps  xmm0, xmm3             ; Save for next iteration        
        add     ecx, 20H               ; Loop through negative values up to zero
        jnz     L1        
        ; Set up for edx remaining bytes
        add     esi, edx
        add     edi, edx
        neg     edx
        cmp     edx, -10H
        jg      L2
        ; One more 16-bytes block to move
        movaps  xmm1, [esi+edx+10H]    ; Read next two blocks aligned
        pshufd  xmm1, xmm1, 10010011B
        movss   xmm1, xmm0             ; Moves 4 bytes, leaves remaining bytes unchanged       
        IF nt eq 0
        movdqa  [edi+edx], xmm1        ; Save aligned
        ELSE
        movntdq [edi+edx], xmm1        ; Non-temporal save
        ENDIF
        add     edx, 10H        
L2:     ; Get src pointer back to misaligned state
        add     esi, eax
        ; Move remaining 0 - 15 bytes, unaligned
        jmp     C200
ENDM 

; Code for each src alignment, Suppl.SSE3 instruction set:
; Make separate code for each alignment u because the palignr instruction
; has the shift count as a constant:

MOVE_UNALIGNED_SSSE3 MACRO u
; Move ecx + edx bytes of data
; Source is misaligned. (src-dest) modulo 16 = u
; eax = u
; esi = src - u = nearest preceding 16-bytes boundary
; edi = dest (aligned)
; ecx = - (count rounded down to nearest divisible by 32)
; edx = remaining bytes to move after loop
LOCAL L1, L2
        movdqa  xmm0, [esi+ecx]        ; Read from nearest preceding 16B boundary
        
L1:     ; Loop. ecx has negative index from the end, counting up to zero
        movdqa  xmm2, [esi+ecx+10H]    ; Read next two blocks
        movdqa  xmm3, [esi+ecx+20H]
        movdqa  xmm1, xmm0             ; Save xmm0
        movdqa  xmm0, xmm3             ; Save for next iteration
        palignr xmm3, xmm2, u          ; Combine parts into aligned block
        palignr xmm2, xmm1, u          ; Combine parts into aligned block
        movdqa  [edi+ecx], xmm2        ; Save aligned
        movdqa  [edi+ecx+10H], xmm3    ; Save aligned
        add     ecx, 20H
        jnz     L1
        
        ; Set up for edx remaining bytes
        add     esi, edx
        add     edi, edx
        neg     edx
        cmp     edx, -10H
        jg      L2
        ; One more 16-bytes block to move
        movdqa  xmm2, [esi+edx+10H]
        palignr xmm2, xmm0, u
        movdqa  [edi+edx], xmm2
        add     edx, 10H        
L2:     ; Get src pointer back to misaligned state
        add     esi, eax
        ; Move remaining 0 - 15 bytes
        jmp     C200
ENDM        

; Make 15 instances of SSE2 macro for each value of the alignment u.
; These are pointed to by the jump table AlignmentDispatchSSE2 below
; (aligns are inserted manually to minimize the number of 16-bytes
; boundaries inside loops)

D101:   MOVE_UNALIGNED_SSE2 1,   0
D102:   MOVE_UNALIGNED_SSE2 2,   0
D103:   MOVE_UNALIGNED_SSE2 3,   0
align   16
D104:   MOVE_UNALIGNED_SSE2_4    0
D105:   MOVE_UNALIGNED_SSE2 5,   0
D106:   MOVE_UNALIGNED_SSE2 6,   0
align   8
D107:   MOVE_UNALIGNED_SSE2 7,   0
align   16
D108:   MOVE_UNALIGNED_SSE2_8    0
D109:   MOVE_UNALIGNED_SSE2 9,   0
D10A:   MOVE_UNALIGNED_SSE2 0AH, 0
D10B:   MOVE_UNALIGNED_SSE2 0BH, 0
align   8
D10C:   MOVE_UNALIGNED_SSE2_12   0
D10D:   MOVE_UNALIGNED_SSE2 0DH, 0
D10E:   MOVE_UNALIGNED_SSE2 0EH, 0
D10F:   MOVE_UNALIGNED_SSE2 0FH, 0
        
; Make 15 instances of Suppl-SSE3 macro for each value of the alignment u.
; These are pointed to by the jump table AlignmentDispatchSupSSE3 below

align   8
E101:   MOVE_UNALIGNED_SSSE3 1
E102:   MOVE_UNALIGNED_SSSE3 2
E103:   MOVE_UNALIGNED_SSSE3 3
E104:   MOVE_UNALIGNED_SSSE3 4
align   8
E105:   MOVE_UNALIGNED_SSSE3 5
E106:   MOVE_UNALIGNED_SSSE3 6
E107:   MOVE_UNALIGNED_SSSE3 7
E108:   MOVE_UNALIGNED_SSSE3 8
align   8
E109:   MOVE_UNALIGNED_SSSE3 9
E10A:   MOVE_UNALIGNED_SSSE3 0AH
E10B:   MOVE_UNALIGNED_SSSE3 0BH
E10C:   MOVE_UNALIGNED_SSSE3 0CH
align   8
E10D:   MOVE_UNALIGNED_SSSE3 0DH
E10E:   MOVE_UNALIGNED_SSSE3 0EH
E10F:   MOVE_UNALIGNED_SSSE3 0FH

; Codes for non-temporal move. Aligned case first

F100:   ; Non-temporal move, src and dest have same alignment.
        ; Loop. ecx has negative index from the end, counting up to zero
        movaps  xmm0, [esi+ecx]        ; Read
        movaps  xmm1, [esi+ecx+10H]
        movntps [edi+ecx], xmm0        ; Write non-temporal (bypass cache)
        movntps [edi+ecx+10H], xmm1
        add     ecx, 20H
        jnz     F100                   ; Loop through negative ecx up to zero
                
        ; Move the remaining edx bytes (0 - 31):
        add     esi, edx
        add     edi, edx
        neg     edx
        jz      C500                   ; Skip if no more data
        ; Check if we can more one more 16-bytes block
        cmp     edx, -10H
        jg      C200
        ; move 16 bytes, aligned
        movaps  xmm0, [esi+edx]
        movntps [edi+edx], xmm0
        add     edx, 10H
        ; move the remaining 0 - 15 bytes
        jmp     C200

; Make 15 instances of MOVE_UNALIGNED_SSE2 macro for each value of 
; the alignment u.
; These are pointed to by the jump table AlignmentDispatchNT below

F101:   MOVE_UNALIGNED_SSE2 1,   1
F102:   MOVE_UNALIGNED_SSE2 2,   1
F103:   MOVE_UNALIGNED_SSE2 3,   1
F104:   MOVE_UNALIGNED_SSE2_4    1
F105:   MOVE_UNALIGNED_SSE2 5,   1
F106:   MOVE_UNALIGNED_SSE2 6,   1
F107:   MOVE_UNALIGNED_SSE2 7,   1
F108:   MOVE_UNALIGNED_SSE2_8    1
F109:   MOVE_UNALIGNED_SSE2 9,   1
F10A:   MOVE_UNALIGNED_SSE2 0AH, 1
F10B:   MOVE_UNALIGNED_SSE2 0BH, 1
F10C:   MOVE_UNALIGNED_SSE2_12   1
F10D:   MOVE_UNALIGNED_SSE2 0DH, 1
F10E:   MOVE_UNALIGNED_SSE2 0EH, 1
F10F:   MOVE_UNALIGNED_SSE2 0FH, 1

IFDEF   POSITIONINDEPENDENT
get_thunk_ebx: ; load caller address into ebx for position-independent code
        mov ebx, [esp]
        ret
ENDIF

; 80386 version used when SSE2 not supported:
memcpy386:
; edi = dest
; esi = src
; ecx = count
        cld
        cmp     ecx, 8
        jb      G500
G100:   test    edi, 1
        jz      G200
        movsb
        dec     ecx
G200:   test    edi, 2
        jz      G300
        movsw
        sub     ecx, 2
G300:   ; edi is aligned now
        mov     edx, ecx
        shr     ecx, 2
        rep     movsd                  ; move 4 bytes at a time
        mov     ecx, edx
        and     ecx, 3
        rep     movsb                  ; move remaining 0-3 bytes
        RETURNM
        
G500:   ; count < 8. Move one byte at a time
        rep     movsb                  ; move count bytes
        RETURNM
        
; CPU dispatching for memcpy. This is executed only once
memcpyCPUDispatch:
IFNDEF   POSITIONINDEPENDENT
        pushad
        call    _InstructionSet
        ; Point to generic version of memcpy
        mov     [memcpyDispatch], offset memcpy386
        cmp     eax, 4                 ; check SSE2
        jb      M100
        ; SSE2 supported
        ; Point to SSE2 and later version of memcpy
        mov     [memcpyDispatch], offset memcpySSE2
        cmp     eax, 6                 ; check Suppl-SSE3
        jb      M100
        ; Suppl-SSE3 supported
        ; Replace alignment dispatch table with Suppl-SSE3 version        
        mov     esi, offset AlignmentDispatchSupSSE3
        mov     edi, offset AlignmentDispatchSSE2
        mov     ecx, 16
        cld
        rep     movsd
M100:   popad
        ; Continue in appropriate version of memcpy
        jmp     [memcpyDispatch]

ELSE    ; Position-independent version
        pushad
        call    _InstructionSet
        
; Make the following instruction with address relative to RP:
; lea edx, [ebx-RP+memcpyCPUVersion]
PICREFERENCE memcpyCPUVersion, RP, 8DH, 93H
; Now edx points to memcpyCPUVersion. Use edx as reference to data segment

        mov     byte ptr [edx], 1      ; Indicate generic version
        cmp     eax, 4                 ; check SSE2
        jb      M100
        ; SSE2 supported
        mov     byte ptr [edx], 2      ; Indicate SSE2 or later version
        cmp     eax, 6                 ; check Suppl-SSE3
        jb      M100
        ; Suppl-SSE3 supported
        ; Replace alignment dispatch table with Suppl-SSE3 version        
        lea     esi, [edx][AlignmentDispatchSupSSE3-memcpyCPUVersion]
        lea     edi, [edx][AlignmentDispatchSSE2-memcpyCPUVersion]
        mov     ecx, 16
        cld
        rep     movsd
M100:   popad
        jmp     A020                   ; Go back and dispatch
        
ENDIF


; Data segment must be included in function namespace
.data

; Jump tables for alignments 0 - 15:
; The CPU dispatcher replaces AlignmentDispatchSSE2 with 
; AlignmentDispatchSupSSE3 if Suppl-SSE3 is supported
; RP = reference point if position-independent code, otherwise RP = 0

AlignmentDispatch label dword
; Code pointer for each alignment for SSE2 instruction set
AlignmentDispatchSSE2 label dword
DD C100-RP, D101-RP, D102-RP, D103-RP, D104-RP, D105-RP, D106-RP, D107-RP
DD D108-RP, D109-RP, D10A-RP, D10B-RP, D10C-RP, D10D-RP, D10E-RP, D10F-RP

; Code pointer for each alignment for Suppl.SSE3 instruction set
AlignmentDispatchSupSSE3 label dword
DD C100-RP, E101-RP, E102-RP, E103-RP, E104-RP, E105-RP, E106-RP, E107-RP
DD E108-RP, E109-RP, E10A-RP, E10B-RP, E10C-RP, E10D-RP, E10E-RP, E10F-RP

; Code pointer for each alignment for non-temporal store
AlignmentDispatchNT label dword
DD F100-RP, F101-RP, F102-RP, F103-RP, F104-RP, F105-RP, F106-RP, F107-RP
DD F108-RP, F109-RP, F10A-RP, F10B-RP, F10C-RP, F10D-RP, F10E-RP, F10F-RP

IFNDEF  POSITIONINDEPENDENT
; Pointer to appropriate version.
; This initially points to memcpyCPUDispatch. memcpyCPUDispatch will
; change this to the appropriate version of memcpy, so that
; memcpyCPUDispatch is only executed once:
memcpyDispatch DD memcpyCPUDispatch
ELSE
; CPU version: 0=unknown, 1=80386, 2=SSE2 or later
memcpyCPUVersion DD 0
ENDIF

; Bypass cache by using non-temporal moves if count > _CacheBypassLimit
; The optimal value of _CacheBypassLimit is difficult to estimate, but
; a reasonable value is half the size of the largest cache:
_CacheBypassLimit DD 400000H             ; 400000H = 4 Megabytes

.code
_A_memcpy ENDP                           ; End of function namespace

END
